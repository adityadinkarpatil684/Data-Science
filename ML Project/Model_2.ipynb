{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99da96d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a01e3d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"loan_preprocess_dataset.csv\")\n",
    "\n",
    "# Ensure target column is clean (remove extra spaces if any)\n",
    "df['loan_status'] = df['loan_status'].str.strip()\n",
    "\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(\"loan_status\", axis=1)\n",
    "y = df[\"loan_status\"]\n",
    "\n",
    "# Convert categorical target to numeric (0/1)\n",
    "y = y.map({\"Rejected\": 0, \"Approved\": 1})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36766645",
   "metadata": {},
   "source": [
    "# Working on Selected Model's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c1d718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:37:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:37:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:37:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Cross-Validation Results (5-Fold):\n",
      "\n",
      "                     accuracy  precision  recall      f1\n",
      "Logistic Regression    0.9154     0.9325  0.9319  0.9321\n",
      "Random Forest          0.9803     0.9809  0.9877  0.9843\n",
      "XGBoost                0.9847     0.9847  0.9908  0.9877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:37:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:37:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "# 1. Define Models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "}\n",
    "\n",
    "# 2. Define k-Fold CV (Stratified to preserve class balance)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 3. Define Metrics\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"precision\": make_scorer(precision_score),\n",
    "    \"recall\": make_scorer(recall_score),\n",
    "    \"f1\": make_scorer(f1_score)\n",
    "}\n",
    "\n",
    "# 4. Run CV for Each Model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    scores = cross_validate(model, X, y, cv=cv, scoring=scoring, return_train_score=False)\n",
    "    results[name] = {metric: np.mean(scores[f'test_{metric}']) for metric in scoring.keys()}\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nðŸ“Š Cross-Validation Results (5-Fold):\\n\")\n",
    "print(results_df.round(4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ec184e",
   "metadata": {},
   "source": [
    "# Hyper-parameter Tuning Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64d1463",
   "metadata": {},
   "source": [
    "# Tuning Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0383dd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-24 19:37:12,111] A new study created in memory with name: no-name-4263a6b9-be0c-493f-9d70-ead1e6c40843\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-24 19:37:12,734] Trial 0 finished with value: 0.6321164092984373 and parameters: {'penalty': 'elasticnet', 'max_iter': 341, 'C': 3.0760946264885773, 'l1_ratio': 0.5403839239107414}. Best is trial 0 with value: 0.6321164092984373.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-24 19:37:14,350] Trial 1 finished with value: 0.6470966084275437 and parameters: {'penalty': 'elasticnet', 'max_iter': 873, 'C': 0.1687711317913671, 'l1_ratio': 0.416795615844022}. Best is trial 1 with value: 0.6470966084275437.\n",
      "[I 2025-08-24 19:37:14,382] Trial 2 finished with value: 0.7840081034902536 and parameters: {'penalty': 'l1', 'max_iter': 1637, 'C': 0.0016331808516922292}. Best is trial 2 with value: 0.7840081034902536.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:1186: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:1186: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:1186: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2025-08-24 19:37:15,853] Trial 3 finished with value: 0.9143202957849353 and parameters: {'penalty': None, 'max_iter': 945, 'C': 4.460040190970332}. Best is trial 3 with value: 0.9143202957849353.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-24 19:37:17,884] Trial 4 finished with value: 0.6617775860174394 and parameters: {'penalty': 'elasticnet', 'max_iter': 1220, 'C': 0.0021780589464547528, 'l1_ratio': 0.22022197296736878}. Best is trial 3 with value: 0.9143202957849353.\n",
      "[I 2025-08-24 19:37:18,818] Trial 5 finished with value: 0.9176141566531141 and parameters: {'penalty': 'l2', 'max_iter': 841, 'C': 0.09245518044841981}. Best is trial 5 with value: 0.9176141566531141.\n",
      "[I 2025-08-24 19:37:18,852] Trial 6 finished with value: 0.9131217936701851 and parameters: {'penalty': 'l1', 'max_iter': 100, 'C': 0.020319577829981884}. Best is trial 5 with value: 0.9176141566531141.\n",
      "[I 2025-08-24 19:37:19,749] Trial 7 finished with value: 0.9170159829010108 and parameters: {'penalty': 'l2', 'max_iter': 1371, 'C': 0.0348031707675556}. Best is trial 5 with value: 0.9176141566531141.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-08-24 19:37:20,481] Trial 8 finished with value: 0.9155182592470498 and parameters: {'penalty': 'l2', 'max_iter': 464, 'C': 4.459709214311536}. Best is trial 5 with value: 0.9176141566531141.\n",
      "[I 2025-08-24 19:37:20,523] Trial 9 finished with value: 0.9143202957849353 and parameters: {'penalty': 'l1', 'max_iter': 948, 'C': 8.261220065192473}. Best is trial 5 with value: 0.9176141566531141.\n",
      "[I 2025-08-24 19:37:21,395] Trial 10 finished with value: 0.915517451268096 and parameters: {'penalty': 'l2', 'max_iter': 1832, 'C': 0.35767874362553}. Best is trial 5 with value: 0.9176141566531141.\n",
      "[I 2025-08-24 19:37:22,184] Trial 11 finished with value: 0.9170165215536464 and parameters: {'penalty': 'l2', 'max_iter': 1427, 'C': 0.026337122639874724}. Best is trial 5 with value: 0.9176141566531141.\n",
      "[I 2025-08-24 19:37:22,880] Trial 12 finished with value: 0.914619786650464 and parameters: {'penalty': 'l2', 'max_iter': 1475, 'C': 0.014620021187049864}. Best is trial 5 with value: 0.9176141566531141.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-08-24 19:37:23,933] Trial 13 finished with value: 0.9170154442483748 and parameters: {'penalty': 'l2', 'max_iter': 656, 'C': 0.41101449069468227}. Best is trial 5 with value: 0.9176141566531141.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:1186: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:1186: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:1186: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2025-08-24 19:37:25,566] Trial 14 finished with value: 0.9143202957849353 and parameters: {'penalty': None, 'max_iter': 1916, 'C': 0.05801185947770628}. Best is trial 5 with value: 0.9176141566531141.\n",
      "[I 2025-08-24 19:37:26,040] Trial 15 finished with value: 0.9065313786706484 and parameters: {'penalty': 'l2', 'max_iter': 1196, 'C': 0.0056016031129325775}. Best is trial 5 with value: 0.9176141566531141.\n",
      "[I 2025-08-24 19:37:26,990] Trial 16 finished with value: 0.9167162227091642 and parameters: {'penalty': 'l2', 'max_iter': 685, 'C': 0.13240750941947516}. Best is trial 5 with value: 0.9176141566531141.\n",
      "[I 2025-08-24 19:37:28,113] Trial 17 finished with value: 0.9164164625173177 and parameters: {'penalty': 'l2', 'max_iter': 1551, 'C': 0.7339365817956831}. Best is trial 5 with value: 0.9176141566531141.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:1186: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:1186: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:1186: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2025-08-24 19:37:29,717] Trial 18 finished with value: 0.9143202957849353 and parameters: {'penalty': None, 'max_iter': 1199, 'C': 0.007241085417755308}. Best is trial 5 with value: 0.9176141566531141.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-08-24 19:37:30,851] Trial 19 finished with value: 0.9149190081896746 and parameters: {'penalty': 'l2', 'max_iter': 720, 'C': 0.9879845131965449}. Best is trial 5 with value: 0.9176141566531141.\n",
      "[I 2025-08-24 19:37:32,121] Trial 20 finished with value: 0.9179136475186428 and parameters: {'penalty': 'l2', 'max_iter': 1720, 'C': 0.07183378842320998}. Best is trial 20 with value: 0.9179136475186428.\n",
      "[I 2025-08-24 19:37:32,889] Trial 21 finished with value: 0.9176141566531141 and parameters: {'penalty': 'l2', 'max_iter': 1664, 'C': 0.06444992825840125}. Best is trial 20 with value: 0.9179136475186428.\n",
      "[I 2025-08-24 19:37:33,642] Trial 22 finished with value: 0.9176141566531141 and parameters: {'penalty': 'l2', 'max_iter': 1725, 'C': 0.08128697902867309}. Best is trial 20 with value: 0.9179136475186428.\n",
      "[I 2025-08-24 19:37:34,956] Trial 23 finished with value: 0.9170157135746928 and parameters: {'penalty': 'l2', 'max_iter': 1814, 'C': 0.1972471518060786}. Best is trial 20 with value: 0.9179136475186428.\n",
      "[I 2025-08-24 19:37:35,824] Trial 24 finished with value: 0.9176138873267963 and parameters: {'penalty': 'l2', 'max_iter': 1621, 'C': 0.05846158684456914}. Best is trial 20 with value: 0.9179136475186428.\n",
      "[I 2025-08-24 19:37:36,393] Trial 25 finished with value: 0.9137210447275604 and parameters: {'penalty': 'l2', 'max_iter': 1942, 'C': 0.01096509295031332}. Best is trial 20 with value: 0.9179136475186428.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:1186: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:1186: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:1186: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2025-08-24 19:37:38,104] Trial 26 finished with value: 0.9143202957849353 and parameters: {'penalty': None, 'max_iter': 1300, 'C': 0.04395128025589043}. Best is trial 20 with value: 0.9179136475186428.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-24 19:37:40,011] Trial 27 finished with value: 0.6533883405405057 and parameters: {'penalty': 'elasticnet', 'max_iter': 1088, 'C': 0.29486279175344327, 'l1_ratio': 0.958210706552435}. Best is trial 20 with value: 0.9179136475186428.\n",
      "[I 2025-08-24 19:37:40,064] Trial 28 finished with value: 0.9149187388633568 and parameters: {'penalty': 'l1', 'max_iter': 1081, 'C': 0.9305666008597163}. Best is trial 20 with value: 0.9179136475186428.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-24 19:37:40,726] Trial 29 finished with value: 0.6339144317968806 and parameters: {'penalty': 'elasticnet', 'max_iter': 360, 'C': 0.0994756220178922, 'l1_ratio': 0.003930617411053938}. Best is trial 20 with value: 0.9179136475186428.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial accuracy: 0.9179136475186428\n",
      "Best hyperparameters: {'penalty': 'l2', 'max_iter': 1720, 'C': 0.07183378842320998}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest values for hyperparameters\n",
    "    penalty = trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\", \"elasticnet\", None])\n",
    "    max_iter = trial.suggest_int(\"max_iter\", 100, 2000)\n",
    "    C = trial.suggest_float(\"C\", 1e-3, 10.0, log=True)\n",
    "\n",
    "    # Solver depends on penalty\n",
    "    if penalty == \"l1\":\n",
    "        solver = \"liblinear\"\n",
    "    elif penalty == \"elasticnet\":\n",
    "        solver = \"saga\"\n",
    "    elif penalty is None:\n",
    "        solver = \"lbfgs\"\n",
    "    else:\n",
    "        solver = \"lbfgs\"\n",
    "\n",
    "    # l1_ratio only if elasticnet\n",
    "    l1_ratio = None\n",
    "    if penalty == \"elasticnet\":\n",
    "        l1_ratio = trial.suggest_float(\"l1_ratio\", 0.0, 1.0)\n",
    "\n",
    "    # Logistic Regression model (no scaler since data is already scaled)\n",
    "    model = LogisticRegression(\n",
    "        penalty=penalty,\n",
    "        solver=solver,\n",
    "        C=C,\n",
    "        l1_ratio=l1_ratio,\n",
    "        max_iter=max_iter,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Cross-validation\n",
    "    score = cross_val_score(model, X_train, y_train, cv=3, scoring=\"accuracy\").mean()\n",
    "    return score\n",
    "\n",
    "# Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print(\"Best trial accuracy:\", study.best_trial.value)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d26d22",
   "metadata": {},
   "source": [
    "# Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6e5ae63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-24 19:37:40,750] A new study created in memory with name: no-name-3821ffd4-5b78-47ae-b6b1-f2476c6c2092\n",
      "[I 2025-08-24 19:37:41,557] Trial 0 finished with value: 0.9718378935665485 and parameters: {'n_estimators': 101, 'max_depth': 7}. Best is trial 0 with value: 0.9718378935665485.\n",
      "[I 2025-08-24 19:37:42,812] Trial 1 finished with value: 0.975135525003178 and parameters: {'n_estimators': 180, 'max_depth': 14}. Best is trial 1 with value: 0.975135525003178.\n",
      "[I 2025-08-24 19:37:43,605] Trial 2 finished with value: 0.9754347465423887 and parameters: {'n_estimators': 108, 'max_depth': 22}. Best is trial 2 with value: 0.9754347465423887.\n",
      "[I 2025-08-24 19:37:47,149] Trial 3 finished with value: 0.9763332191389745 and parameters: {'n_estimators': 483, 'max_depth': 17}. Best is trial 3 with value: 0.9763332191389745.\n",
      "[I 2025-08-24 19:37:48,809] Trial 4 finished with value: 0.9763334884652926 and parameters: {'n_estimators': 228, 'max_depth': 13}. Best is trial 4 with value: 0.9763334884652926.\n",
      "[I 2025-08-24 19:37:51,129] Trial 5 finished with value: 0.976033728273446 and parameters: {'n_estimators': 348, 'max_depth': 14}. Best is trial 4 with value: 0.9763334884652926.\n",
      "[I 2025-08-24 19:37:53,256] Trial 6 finished with value: 0.970639660778116 and parameters: {'n_estimators': 293, 'max_depth': 5}. Best is trial 4 with value: 0.9763334884652926.\n",
      "[I 2025-08-24 19:37:54,274] Trial 7 finished with value: 0.9757345067342352 and parameters: {'n_estimators': 140, 'max_depth': 25}. Best is trial 4 with value: 0.9763334884652926.\n",
      "[I 2025-08-24 19:37:57,382] Trial 8 finished with value: 0.9760334589471281 and parameters: {'n_estimators': 410, 'max_depth': 24}. Best is trial 4 with value: 0.9763334884652926.\n",
      "[I 2025-08-24 19:37:59,701] Trial 9 finished with value: 0.9745351966405312 and parameters: {'n_estimators': 340, 'max_depth': 12}. Best is trial 4 with value: 0.9763334884652926.\n",
      "[I 2025-08-24 19:38:01,194] Trial 10 finished with value: 0.9766329793308212 and parameters: {'n_estimators': 217, 'max_depth': 30}. Best is trial 10 with value: 0.9766329793308212.\n",
      "[I 2025-08-24 19:38:02,994] Trial 11 finished with value: 0.9760339975997638 and parameters: {'n_estimators': 216, 'max_depth': 30}. Best is trial 10 with value: 0.9766329793308212.\n",
      "[I 2025-08-24 19:38:04,563] Trial 12 finished with value: 0.9748349568323778 and parameters: {'n_estimators': 232, 'max_depth': 9}. Best is trial 10 with value: 0.9766329793308212.\n",
      "[I 2025-08-24 19:38:06,727] Trial 13 finished with value: 0.9589560157803675 and parameters: {'n_estimators': 297, 'max_depth': 2}. Best is trial 10 with value: 0.9766329793308212.\n",
      "[I 2025-08-24 19:38:08,005] Trial 14 finished with value: 0.9757345067342352 and parameters: {'n_estimators': 186, 'max_depth': 19}. Best is trial 10 with value: 0.9766329793308212.\n",
      "[I 2025-08-24 19:38:09,990] Trial 15 finished with value: 0.9766327100045031 and parameters: {'n_estimators': 252, 'max_depth': 30}. Best is trial 10 with value: 0.9766329793308212.\n",
      "[I 2025-08-24 19:38:10,574] Trial 16 finished with value: 0.9748354954850136 and parameters: {'n_estimators': 73, 'max_depth': 30}. Best is trial 10 with value: 0.9766329793308212.\n",
      "[I 2025-08-24 19:38:12,338] Trial 17 finished with value: 0.9763332191389745 and parameters: {'n_estimators': 260, 'max_depth': 27}. Best is trial 10 with value: 0.9766329793308212.\n",
      "[I 2025-08-24 19:38:15,138] Trial 18 finished with value: 0.976033728273446 and parameters: {'n_estimators': 406, 'max_depth': 21}. Best is trial 10 with value: 0.9766329793308212.\n",
      "[I 2025-08-24 19:38:16,485] Trial 19 finished with value: 0.9757342374079173 and parameters: {'n_estimators': 173, 'max_depth': 27}. Best is trial 10 with value: 0.9766329793308212.\n",
      "[I 2025-08-24 19:38:18,712] Trial 20 finished with value: 0.9754342078897529 and parameters: {'n_estimators': 329, 'max_depth': 28}. Best is trial 10 with value: 0.9766329793308212.\n",
      "[I 2025-08-24 19:38:20,452] Trial 21 finished with value: 0.9766324406781853 and parameters: {'n_estimators': 233, 'max_depth': 18}. Best is trial 10 with value: 0.9766329793308212.\n",
      "[I 2025-08-24 19:38:22,409] Trial 22 finished with value: 0.9766327100045031 and parameters: {'n_estimators': 264, 'max_depth': 24}. Best is trial 10 with value: 0.9766329793308212.\n",
      "[I 2025-08-24 19:38:24,318] Trial 23 finished with value: 0.9757339680815994 and parameters: {'n_estimators': 283, 'max_depth': 25}. Best is trial 10 with value: 0.9766329793308212.\n",
      "[I 2025-08-24 19:38:26,951] Trial 24 finished with value: 0.976033728273446 and parameters: {'n_estimators': 377, 'max_depth': 30}. Best is trial 10 with value: 0.9766329793308212.\n",
      "[I 2025-08-24 19:38:28,737] Trial 25 finished with value: 0.9766327100045031 and parameters: {'n_estimators': 259, 'max_depth': 22}. Best is trial 10 with value: 0.9766329793308212.\n",
      "[I 2025-08-24 19:38:29,833] Trial 26 finished with value: 0.9754347465423887 and parameters: {'n_estimators': 135, 'max_depth': 28}. Best is trial 10 with value: 0.9766329793308212.\n",
      "[I 2025-08-24 19:38:31,229] Trial 27 finished with value: 0.9757342374079173 and parameters: {'n_estimators': 197, 'max_depth': 24}. Best is trial 10 with value: 0.9766329793308212.\n",
      "[I 2025-08-24 19:38:31,677] Trial 28 finished with value: 0.9748357648113316 and parameters: {'n_estimators': 51, 'max_depth': 26}. Best is trial 10 with value: 0.9766329793308212.\n",
      "[I 2025-08-24 19:38:32,731] Trial 29 finished with value: 0.9757342374079173 and parameters: {'n_estimators': 144, 'max_depth': 29}. Best is trial 10 with value: 0.9766329793308212.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial accuracy: 0.9766329793308212\n",
      "Best hyperparameters: {'n_estimators': 217, 'max_depth': 30}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the Optuna objective function\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)  # number of trees\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 30)  # tree depth\n",
    "\n",
    "    # Define model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=42,\n",
    "        n_jobs=-1  # use all cores\n",
    "    )\n",
    "\n",
    "    # Perform CV accuracy\n",
    "    score = cross_val_score(model, X_train, y_train, cv=3, scoring=\"accuracy\").mean()\n",
    "    return score\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Print best result\n",
    "print(f\"Best trial accuracy: {study.best_trial.value}\")\n",
    "print(f\"Best hyperparameters: {study.best_trial.params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f32e95",
   "metadata": {},
   "source": [
    "# XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03d041f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-24 19:38:32,750] A new study created in memory with name: no-name-337aac6e-4ede-4d35-8db8-e810772b39bb\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:33,482] Trial 0 finished with value: 0.9769319315437137 and parameters: {'n_estimators': 275, 'max_depth': 3, 'learning_rate': 0.04646313565412953}. Best is trial 0 with value: 0.9769319315437137.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:34,149] Trial 1 finished with value: 0.9793275891416248 and parameters: {'n_estimators': 487, 'max_depth': 8, 'learning_rate': 0.20072517605480192}. Best is trial 1 with value: 0.9793275891416248.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:35,226] Trial 2 finished with value: 0.9787277994316138 and parameters: {'n_estimators': 397, 'max_depth': 10, 'learning_rate': 0.010945061737324681}. Best is trial 1 with value: 0.9793275891416248.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:35,734] Trial 3 finished with value: 0.9787280687579316 and parameters: {'n_estimators': 154, 'max_depth': 12, 'learning_rate': 0.03195003207365321}. Best is trial 1 with value: 0.9793275891416248.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:36,427] Trial 4 finished with value: 0.9799268401989999 and parameters: {'n_estimators': 263, 'max_depth': 8, 'learning_rate': 0.011374190538088986}. Best is trial 4 with value: 0.9799268401989999.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:36,703] Trial 5 finished with value: 0.9778309427929356 and parameters: {'n_estimators': 179, 'max_depth': 3, 'learning_rate': 0.13556439070904616}. Best is trial 4 with value: 0.9799268401989999.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:37,076] Trial 6 finished with value: 0.968841099627037 and parameters: {'n_estimators': 256, 'max_depth': 3, 'learning_rate': 0.02098584930419371}. Best is trial 4 with value: 0.9799268401989999.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:37,697] Trial 7 finished with value: 0.9790283676024142 and parameters: {'n_estimators': 287, 'max_depth': 6, 'learning_rate': 0.04139310621023869}. Best is trial 4 with value: 0.9799268401989999.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:38,122] Trial 8 finished with value: 0.9736356467386736 and parameters: {'n_estimators': 229, 'max_depth': 4, 'learning_rate': 0.014807379417019739}. Best is trial 4 with value: 0.9799268401989999.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:38,472] Trial 9 finished with value: 0.9796273493334713 and parameters: {'n_estimators': 133, 'max_depth': 13, 'learning_rate': 0.0991585484473371}. Best is trial 4 with value: 0.9799268401989999.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:38,784] Trial 10 finished with value: 0.978428847218721 and parameters: {'n_estimators': 59, 'max_depth': 15, 'learning_rate': 0.08394292277713236}. Best is trial 4 with value: 0.9799268401989999.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:39,040] Trial 11 finished with value: 0.9787286074105676 and parameters: {'n_estimators': 52, 'max_depth': 13, 'learning_rate': 0.08959958912880121}. Best is trial 4 with value: 0.9799268401989999.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:39,528] Trial 12 finished with value: 0.9808258514482215 and parameters: {'n_estimators': 350, 'max_depth': 9, 'learning_rate': 0.27966191200574264}. Best is trial 12 with value: 0.9808258514482215.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:40,025] Trial 13 finished with value: 0.9793281277942606 and parameters: {'n_estimators': 349, 'max_depth': 8, 'learning_rate': 0.2943600203585717}. Best is trial 12 with value: 0.9808258514482215.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:40,972] Trial 14 finished with value: 0.9778298654876637 and parameters: {'n_estimators': 393, 'max_depth': 10, 'learning_rate': 0.024295819420996394}. Best is trial 12 with value: 0.9808258514482215.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:41,630] Trial 15 finished with value: 0.9787286074105676 and parameters: {'n_estimators': 342, 'max_depth': 6, 'learning_rate': 0.0655287348001704}. Best is trial 12 with value: 0.9808258514482215.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:42,271] Trial 16 finished with value: 0.9805258219300571 and parameters: {'n_estimators': 485, 'max_depth': 7, 'learning_rate': 0.29977503485901186}. Best is trial 12 with value: 0.9808258514482215.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:42,917] Trial 17 finished with value: 0.9808253127955856 and parameters: {'n_estimators': 497, 'max_depth': 6, 'learning_rate': 0.268060763699314}. Best is trial 12 with value: 0.9808258514482215.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:43,555] Trial 18 finished with value: 0.9793273198153067 and parameters: {'n_estimators': 457, 'max_depth': 5, 'learning_rate': 0.1960613675370797}. Best is trial 12 with value: 0.9808258514482215.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:44,203] Trial 19 finished with value: 0.9802257924118926 and parameters: {'n_estimators': 425, 'max_depth': 10, 'learning_rate': 0.1704410662102839}. Best is trial 12 with value: 0.9808258514482215.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:44,791] Trial 20 finished with value: 0.9799265708726819 and parameters: {'n_estimators': 330, 'max_depth': 11, 'learning_rate': 0.11697759846040608}. Best is trial 12 with value: 0.9808258514482215.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:45,431] Trial 21 finished with value: 0.9811250729874321 and parameters: {'n_estimators': 498, 'max_depth': 7, 'learning_rate': 0.2658699589289306}. Best is trial 21 with value: 0.9811250729874321.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:46,046] Trial 22 finished with value: 0.9781290870268745 and parameters: {'n_estimators': 440, 'max_depth': 6, 'learning_rate': 0.23127049772970462}. Best is trial 21 with value: 0.9811250729874321.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:46,832] Trial 23 finished with value: 0.9805255526037392 and parameters: {'n_estimators': 495, 'max_depth': 9, 'learning_rate': 0.14923826024366066}. Best is trial 21 with value: 0.9811250729874321.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:47,362] Trial 24 finished with value: 0.979926032220046 and parameters: {'n_estimators': 390, 'max_depth': 7, 'learning_rate': 0.23632968605561613}. Best is trial 21 with value: 0.9811250729874321.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:47,988] Trial 25 finished with value: 0.981125611640068 and parameters: {'n_estimators': 454, 'max_depth': 5, 'learning_rate': 0.248815576263874}. Best is trial 25 with value: 0.981125611640068.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:48,606] Trial 26 finished with value: 0.9787280687579316 and parameters: {'n_estimators': 426, 'max_depth': 5, 'learning_rate': 0.17219166471861574}. Best is trial 25 with value: 0.981125611640068.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:49,323] Trial 27 finished with value: 0.9802266003908464 and parameters: {'n_estimators': 374, 'max_depth': 9, 'learning_rate': 0.06799897819646727}. Best is trial 25 with value: 0.981125611640068.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:49,956] Trial 28 finished with value: 0.9799263015463641 and parameters: {'n_estimators': 460, 'max_depth': 5, 'learning_rate': 0.21877071635388032}. Best is trial 25 with value: 0.981125611640068.\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [19:38:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-24 19:38:50,507] Trial 29 finished with value: 0.9799265708726819 and parameters: {'n_estimators': 307, 'max_depth': 7, 'learning_rate': 0.11955334954473389}. Best is trial 25 with value: 0.981125611640068.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial accuracy: 0.981125611640068\n",
      "Best hyperparameters: {'n_estimators': 454, 'max_depth': 5, 'learning_rate': 0.248815576263874}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest values\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 15)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
    "\n",
    "    # Define model\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Cross-validation score (3-fold, accuracy)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=3, scoring=\"accuracy\").mean()\n",
    "    return score\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Print results\n",
    "print(\"Best trial accuracy:\", study.best_trial.value)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
